import{_ as t,o as e,c as o,S as a}from"./chunks/framework.d62eb0c8.js";const _=JSON.parse('{"title":"Robots.txt - SEO Fields","description":"","frontmatter":{"title":"Robots.txt - SEO Fields","prev":false,"next":false},"headers":[],"relativePath":"robots.md","filePath":"robots.md"}'),s={name:"robots.md"},i=a('<h1 id="robots-txt" tabindex="-1">Robots.txt <a class="header-anchor" href="#robots-txt" aria-label="Permalink to &quot;Robots.txt&quot;">​</a></h1><h3 id="default" tabindex="-1">Default <a class="header-anchor" href="#default" aria-label="Permalink to &quot;Default&quot;">​</a></h3><p>When you install the plugin, it enabled the robots.txt by default, using <a href="https://github.com/studioespresso/craft-seo-fields/blob/master/src/templates/_placeholder/_robots.twig" target="_blank" rel="noreferrer">this template</a>.</p><p>Based on the environment you&#39;ve set in your site&#39;s <code>.env</code> file, it will only allow indexing on <code>live</code> or <code>production</code> environments and block indexing on all others.</p><p>You can modify the template with your own enviroments, conditions and settings in the CP.</p><p>⚠️ Note that this will not stop Google from indexing your staging/dev site if and when someone else has a link to it. Always add another level of authentication to make sure Google can&#39;t index it.</p><h3 id="multisite" tabindex="-1">Multisite <a class="header-anchor" href="#multisite" aria-label="Permalink to &quot;Multisite&quot;">​</a></h3><p>By default, each site in your Craft install will get the same <code>robots.txt</code>. If you need the option to change these per site, you can add <code>&quot;robotsPerSite&quot; =&gt; true</code> to <code>config/seo-fields.php</code>.</p><p>With that set, refresh the robots settings page and you&#39;ll see a sites dropdown at the top so you can switch sites and save a robots.txt for each.</p>',9),n=[i];function r(l,d,c,h,u,p){return e(),o("div",null,n)}const b=t(s,[["render",r]]);export{_ as __pageData,b as default};
